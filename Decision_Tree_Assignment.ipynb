{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df3f9fe1",
      "metadata": {
        "id": "df3f9fe1"
      },
      "source": [
        "# Decision Tree Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99808570",
      "metadata": {
        "id": "99808570"
      },
      "source": [
        "## Theoretical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf17f367",
      "metadata": {
        "id": "cf17f367"
      },
      "source": [
        "**Q1. What is a Decision Tree, and how does it work**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9993e0f",
      "metadata": {
        "id": "d9993e0f"
      },
      "source": [
        "A Decision Tree is a flowchart-like structure used for classification and regression. It splits the dataset into subsets based on the value of input features. At each internal node, a feature is selected that best splits the data based on a criterion such as Gini Impurity or Entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b50efab",
      "metadata": {
        "id": "0b50efab"
      },
      "source": [
        "**Q2. What are impurity measures in Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea4c4a1",
      "metadata": {
        "id": "bea4c4a1"
      },
      "source": [
        "Impurity measures determine how well a split separates the data. Common measures include Gini Impurity and Entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5973f83",
      "metadata": {
        "id": "f5973f83"
      },
      "source": [
        "**Q3. What is the mathematical formula for Gini Impurity**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57540a1a",
      "metadata": {
        "id": "57540a1a"
      },
      "source": [
        "$$Gini = 1 - \\sum_{i=1}^{n} p_i^2$$ where $p_i$ is the probability of class $i$ in a node."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86bce16b",
      "metadata": {
        "id": "86bce16b"
      },
      "source": [
        "**Q4. What is the mathematical formula for Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12e9a9f",
      "metadata": {
        "id": "e12e9a9f"
      },
      "source": [
        "$$Entropy = -\\sum_{i=1}^{n} p_i \\log_2(p_i)$$ where $p_i$ is the probability of class $i$ in a node."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa36248",
      "metadata": {
        "id": "9aa36248"
      },
      "source": [
        "**Q5. What is Information Gain, and how is it used in Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "852a30a5",
      "metadata": {
        "id": "852a30a5"
      },
      "source": [
        "Information Gain is the reduction in impurity after a dataset is split on an attribute. It is calculated as the difference between the impurity of the parent node and the weighted impurity of the child nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd63f0f1",
      "metadata": {
        "id": "dd63f0f1"
      },
      "source": [
        "**Q6. What is the difference between Gini Impurity and Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df28dc0",
      "metadata": {
        "id": "5df28dc0"
      },
      "source": [
        "Both are measures of impurity, but Gini tends to be simpler and faster to compute. Entropy involves logarithmic computation and may provide more balanced splits."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ac1349",
      "metadata": {
        "id": "53ac1349"
      },
      "source": [
        "**Q7. What is the mathematical explanation behind Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e6d8eb",
      "metadata": {
        "id": "77e6d8eb"
      },
      "source": [
        "Decision Trees build a model by selecting splits that maximize Information Gain or minimize impurity. This is done recursively until a stopping condition is met (like depth, min samples, etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e57336",
      "metadata": {
        "id": "89e57336"
      },
      "source": [
        "**Q8. What is Pre-Pruning in Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54698f0d",
      "metadata": {
        "id": "54698f0d"
      },
      "source": [
        "Pre-Pruning stops the tree growth early using conditions like max depth, min samples split, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f09c3d",
      "metadata": {
        "id": "46f09c3d"
      },
      "source": [
        "**Q9. What is Post-Pruning in Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6558742c",
      "metadata": {
        "id": "6558742c"
      },
      "source": [
        "Post-Pruning allows the tree to grow fully, and then prunes back branches that do not improve generalization using a cost-complexity approach."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6260dc",
      "metadata": {
        "id": "bd6260dc"
      },
      "source": [
        "**Q10. What is the difference between Pre-Pruning and Post-Pruning**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2676d830",
      "metadata": {
        "id": "2676d830"
      },
      "source": [
        "Pre-Pruning prevents overfitting during training, while Post-Pruning removes overfit branches after training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b26b8baa",
      "metadata": {
        "id": "b26b8baa"
      },
      "source": [
        "**Q11. What is a Decision Tree Regressor**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fb86ad1",
      "metadata": {
        "id": "0fb86ad1"
      },
      "source": [
        "A Decision Tree Regressor is a type of decision tree used for regression tasks, predicting continuous values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374a11a9",
      "metadata": {
        "id": "374a11a9"
      },
      "source": [
        "**Q12. What are the advantages and disadvantages of Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2459254",
      "metadata": {
        "id": "f2459254"
      },
      "source": [
        "- Advantages: Easy to interpret, handles both numerical and categorical data, non-parametric.\n",
        "- Disadvantages: Prone to overfitting, unstable with small variations in data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41730144",
      "metadata": {
        "id": "41730144"
      },
      "source": [
        "**Q13. How does a Decision Tree handle missing values**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d371ddfc",
      "metadata": {
        "id": "d371ddfc"
      },
      "source": [
        "Some implementations allow handling missing values by surrogate splits or skipping features with missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b8d9ced",
      "metadata": {
        "id": "3b8d9ced"
      },
      "source": [
        "**Q14. How does a Decision Tree handle categorical features**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e345e4",
      "metadata": {
        "id": "e4e345e4"
      },
      "source": [
        "Categorical features can be split based on subsets of categories. Most libraries convert them to dummy variables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6627541f",
      "metadata": {
        "id": "6627541f"
      },
      "source": [
        "**Q15. What are some real-world applications of Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "494f0ded",
      "metadata": {
        "id": "494f0ded"
      },
      "source": [
        "Loan approval, medical diagnosis, customer segmentation, fraud detection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25486c0f",
      "metadata": {
        "id": "25486c0f"
      },
      "source": [
        "## Practical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc09136",
      "metadata": {
        "id": "1bc09136"
      },
      "source": [
        "**Q1. Train a Decision Tree Classifier on the Iris dataset and print the model accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413fb92a",
      "metadata": {
        "id": "413fb92a"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b93343e",
      "metadata": {
        "id": "3b93343e"
      },
      "source": [
        "**Q2. Train a Decision Tree Classifier using Gini Impurity and print the feature importances**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "999d1f3c",
      "metadata": {
        "id": "999d1f3c"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f6731b9",
      "metadata": {
        "id": "8f6731b9"
      },
      "source": [
        "**Q3. Train a Decision Tree Classifier using Entropy and print the model accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75fb1c5a",
      "metadata": {
        "id": "75fb1c5a"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier(criterion='entropy')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy (Entropy):\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e467c2d",
      "metadata": {
        "id": "5e467c2d"
      },
      "source": [
        "**Q4. Train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b831523",
      "metadata": {
        "id": "7b831523"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a640bc42",
      "metadata": {
        "id": "a640bc42"
      },
      "source": [
        "**Q5. Train a Decision Tree Classifier and visualize the tree using graphviz**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19be3fce",
      "metadata": {
        "id": "19be3fce"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "dot_data = export_graphviz(model, out_file=None,\n",
        "                           feature_names=load_iris().feature_names,\n",
        "                           class_names=load_iris().target_names,\n",
        "                           filled=True, rounded=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")  # Saves to decision_tree.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193caa17",
      "metadata": {
        "id": "193caa17"
      },
      "source": [
        "**Q6. Train a Decision Tree Classifier with max_depth=3 and compare accuracy with a full tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8cadabd",
      "metadata": {
        "id": "b8cadabd"
      },
      "outputs": [],
      "source": [
        "model1 = DecisionTreeClassifier(max_depth=3)\n",
        "model1.fit(X_train, y_train)\n",
        "print(\"Accuracy (depth=3):\", model1.score(X_test, y_test))\n",
        "\n",
        "model2 = DecisionTreeClassifier()\n",
        "model2.fit(X_train, y_train)\n",
        "print(\"Accuracy (full tree):\", model2.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4fe4d2",
      "metadata": {
        "id": "7a4fe4d2"
      },
      "source": [
        "**Q7. Train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12cb60f8",
      "metadata": {
        "id": "12cb60f8"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier(min_samples_split=5)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy (min_samples_split=5):\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f510fbd",
      "metadata": {
        "id": "2f510fbd"
      },
      "source": [
        "**Q8. Apply feature scaling before training a Decision Tree Classifier and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd264f22",
      "metadata": {
        "id": "cd264f22"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_scaled, y)\n",
        "\n",
        "model_scaled = DecisionTreeClassifier()\n",
        "model_scaled.fit(X_train_s, y_train_s)\n",
        "print(\"Accuracy with scaling:\", model_scaled.score(X_test_s, y_test_s))\n",
        "\n",
        "model_unscaled = DecisionTreeClassifier()\n",
        "model_unscaled.fit(X_train, y_train)\n",
        "print(\"Accuracy without scaling:\", model_unscaled.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c2461d0",
      "metadata": {
        "id": "0c2461d0"
      },
      "source": [
        "**Q9. Train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc57d8aa",
      "metadata": {
        "id": "fc57d8aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "ovr_model = OneVsRestClassifier(DecisionTreeClassifier())\n",
        "ovr_model.fit(X_train, y_train)\n",
        "print(\"OvR Accuracy:\", ovr_model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46d0cd9d",
      "metadata": {
        "id": "46d0cd9d"
      },
      "source": [
        "**Q10. Train a Decision Tree Classifier and display the feature importance scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c2f8bc",
      "metadata": {
        "id": "d3c2f8bc"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00d93476",
      "metadata": {
        "id": "00d93476"
      },
      "source": [
        "**Q11. Train a Decision Tree Regressor with max_depth=5 and compare its performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ba4419",
      "metadata": {
        "id": "43ba4419"
      },
      "outputs": [],
      "source": [
        "reg1 = DecisionTreeRegressor(max_depth=5)\n",
        "reg1.fit(X_train, y_train)\n",
        "print(\"MSE (depth=5):\", mean_squared_error(y_test, reg1.predict(X_test)))\n",
        "\n",
        "reg2 = DecisionTreeRegressor()\n",
        "reg2.fit(X_train, y_train)\n",
        "print(\"MSE (unrestricted):\", mean_squared_error(y_test, reg2.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a00be2f5",
      "metadata": {
        "id": "a00be2f5"
      },
      "source": [
        "**Q12. Train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f1b269",
      "metadata": {
        "id": "94f1b269"
      },
      "outputs": [],
      "source": [
        "path = model.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(f\"Accuracy with ccp_alpha={ccp_alpha:.5f}: {clf.score(X_test, y_test):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a722ac2",
      "metadata": {
        "id": "5a722ac2"
      },
      "source": [
        "**Q13. Train a Decision Tree Classifier and evaluate using Precision, Recall, and F1-Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2859af",
      "metadata": {
        "id": "8e2859af"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25549b6",
      "metadata": {
        "id": "c25549b6"
      },
      "source": [
        "**Q14. Train a Decision Tree Classifier and visualize the confusion matrix using seaborn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca65d403",
      "metadata": {
        "id": "ca65d403"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, model.predict(X_test))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a51aa864",
      "metadata": {
        "id": "a51aa864"
      },
      "source": [
        "**Q15. Train a Decision Tree Classifier and use GridSearchCV for tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8ec4657",
      "metadata": {
        "id": "e8ec4657"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'max_depth': [3, 5, 7], 'min_samples_split': [2, 5, 10]}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}