{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6724cbf1",
      "metadata": {
        "id": "6724cbf1"
      },
      "source": [
        "# ðŸ“˜ KNN & PCA Assignment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70385a7b",
      "metadata": {
        "id": "70385a7b"
      },
      "source": [
        "## ðŸ§  Theoretical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb769dd",
      "metadata": {
        "id": "4cb769dd"
      },
      "source": [
        "**Q1. What is K-Nearest Neighbors (KNN) and how does it work**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aefc7afc",
      "metadata": {
        "id": "aefc7afc"
      },
      "source": [
        "KNN is a supervised learning algorithm used for classification and regression. It predicts the output for a data point by identifying the 'k' closest data points (neighbors) from the training set and taking a majority vote (for classification) or averaging (for regression). It uses a distance metric (like Euclidean) to find neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb95d239",
      "metadata": {
        "id": "bb95d239"
      },
      "source": [
        "**Q2. What is the difference between KNN Classification and KNN Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27daa9f2",
      "metadata": {
        "id": "27daa9f2"
      },
      "source": [
        "KNN Classification predicts a discrete class label using the majority class among neighbors, while KNN Regression predicts a continuous value by averaging the outputs of neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80a059fc",
      "metadata": {
        "id": "80a059fc"
      },
      "source": [
        "**Q3. What is the role of the distance metric in KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11224f4a",
      "metadata": {
        "id": "11224f4a"
      },
      "source": [
        "The distance metric (like Euclidean, Manhattan) determines how closeness between points is calculated, affecting which data points are considered neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa675c3a",
      "metadata": {
        "id": "fa675c3a"
      },
      "source": [
        "**Q4. What is the Curse of Dimensionality in KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f9d3e4b",
      "metadata": {
        "id": "7f9d3e4b"
      },
      "source": [
        "As the number of features (dimensions) increases, the distance between points becomes less meaningful, making it harder for KNN to find relevant neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a6f507",
      "metadata": {
        "id": "44a6f507"
      },
      "source": [
        "**Q5. How can we choose the best value of K in KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94cebd8f",
      "metadata": {
        "id": "94cebd8f"
      },
      "source": [
        "The best value of K can be found using cross-validation â€” trying different K values and choosing the one with the highest validation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72046c4",
      "metadata": {
        "id": "e72046c4"
      },
      "source": [
        "**Q6. What are KD Tree and Ball Tree in KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e63fe3e5",
      "metadata": {
        "id": "e63fe3e5"
      },
      "source": [
        "KD Tree and Ball Tree are data structures that speed up the search for nearest neighbors, especially useful for large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9372a999",
      "metadata": {
        "id": "9372a999"
      },
      "source": [
        "**Q7. When should you use KD Tree vs. Ball Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "852ff216",
      "metadata": {
        "id": "852ff216"
      },
      "source": [
        "KD Tree is efficient for low-dimensional data; Ball Tree is better for high-dimensional data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732cc125",
      "metadata": {
        "id": "732cc125"
      },
      "source": [
        "**Q8. What are the disadvantages of KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f0ae9c",
      "metadata": {
        "id": "b8f0ae9c"
      },
      "source": [
        "- Slow prediction for large datasets\n",
        "- Sensitive to irrelevant features and feature scales\n",
        "- Needs scaling\n",
        "- High memory usage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e16c4b5",
      "metadata": {
        "id": "8e16c4b5"
      },
      "source": [
        "**Q9. How does feature scaling affect KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e54dc62",
      "metadata": {
        "id": "0e54dc62"
      },
      "source": [
        "KNN relies on distance, so features with larger ranges can dominate unless features are scaled (standardized or normalized)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3404caf",
      "metadata": {
        "id": "f3404caf"
      },
      "source": [
        "**Q10. What is PCA (Principal Component Analysis)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd82df9",
      "metadata": {
        "id": "0cd82df9"
      },
      "source": [
        "PCA is a dimensionality reduction technique that transforms features into a new set of orthogonal (uncorrelated) components capturing most variance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c06700bb",
      "metadata": {
        "id": "c06700bb"
      },
      "source": [
        "**Q11. How does PCA work**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c151143a",
      "metadata": {
        "id": "c151143a"
      },
      "source": [
        "PCA finds the directions (principal components) that maximize the variance in data and projects the data onto these directions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "819300de",
      "metadata": {
        "id": "819300de"
      },
      "source": [
        "**Q12. What is the geometric intuition behind PCA**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcce8ce",
      "metadata": {
        "id": "4fcce8ce"
      },
      "source": [
        "PCA rotates the coordinate system to align axes with the directions of greatest data variance, reducing dimensionality while preserving as much information as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ebc053f",
      "metadata": {
        "id": "2ebc053f"
      },
      "source": [
        "**Q13. What is the difference between Feature Selection and Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b491d66",
      "metadata": {
        "id": "4b491d66"
      },
      "source": [
        "- Feature Selection chooses a subset of existing features.\n",
        "- Feature Extraction creates new features from the original ones (e.g., PCA)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e15ee67b",
      "metadata": {
        "id": "e15ee67b"
      },
      "source": [
        "**Q14. What are Eigenvalues and Eigenvectors in PCA**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1fc525",
      "metadata": {
        "id": "af1fc525"
      },
      "source": [
        "Eigenvectors determine the directions of principal components; eigenvalues represent the variance captured by each component."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e84ac8",
      "metadata": {
        "id": "95e84ac8"
      },
      "source": [
        "**Q15. How do you decide the number of components to keep in PCA**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30510744",
      "metadata": {
        "id": "30510744"
      },
      "source": [
        "Choose the number of components such that the cumulative explained variance ratio crosses a threshold (e.g., 90%) or use a Scree plot."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f37276",
      "metadata": {
        "id": "16f37276"
      },
      "source": [
        "**Q16. Can PCA be used for classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ad67df",
      "metadata": {
        "id": "e9ad67df"
      },
      "source": [
        "Yes, PCA can be used to reduce dimensionality before classification for improved efficiency and sometimes better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4debe28c",
      "metadata": {
        "id": "4debe28c"
      },
      "source": [
        "**Q17. What are the limitations of PCA**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd274aa1",
      "metadata": {
        "id": "bd274aa1"
      },
      "source": [
        "- Assumes linearity\n",
        "- Reduces interpretability\n",
        "- Sensitive to feature scaling\n",
        "- May discard useful information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc554b20",
      "metadata": {
        "id": "fc554b20"
      },
      "source": [
        "**Q18. How do KNN and PCA complement each other**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34872918",
      "metadata": {
        "id": "34872918"
      },
      "source": [
        "PCA reduces noise and dimensionality, making KNN faster and sometimes more accurate by removing irrelevant features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd396417",
      "metadata": {
        "id": "bd396417"
      },
      "source": [
        "**Q19. How does KNN handle missing values in a dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd090b8",
      "metadata": {
        "id": "3dd090b8"
      },
      "source": [
        "KNN can be used to impute missing values by averaging (or majority vote) of the nearest neighborsâ€™ values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f5487d",
      "metadata": {
        "id": "78f5487d"
      },
      "source": [
        "**Q20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ac1ba9",
      "metadata": {
        "id": "d8ac1ba9"
      },
      "source": [
        "- PCA maximizes variance without considering class labels.\n",
        "- LDA maximizes class separation using label information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f93e3f",
      "metadata": {
        "id": "f2f93e3f"
      },
      "source": [
        "## ðŸ’» Practical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b497c9",
      "metadata": {
        "id": "11b497c9"
      },
      "source": [
        "**Q21. Train a KNN Classifier on the Iris dataset and print model accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f24287",
      "metadata": {
        "id": "15f24287"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d2e28d1",
      "metadata": {
        "id": "9d2e28d1"
      },
      "source": [
        "**Q22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70031ef4",
      "metadata": {
        "id": "70031ef4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=2, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = KNeighborsRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"MSE:\", mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da65a454",
      "metadata": {
        "id": "da65a454"
      },
      "source": [
        "**Q23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9481b1ce",
      "metadata": {
        "id": "9481b1ce"
      },
      "outputs": [],
      "source": [
        "for metric in ['euclidean', 'manhattan']:\n",
        "    model = KNeighborsClassifier(metric=metric)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"{metric.title()} Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcdbb6ed",
      "metadata": {
        "id": "bcdbb6ed"
      },
      "source": [
        "**Q24. Train a KNN Classifier with different values of K and visualize decision boundaries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8af9d956",
      "metadata": {
        "id": "8af9d956"
      },
      "outputs": [],
      "source": [
        "# This task is visual and would typically involve using matplotlib\n",
        "print(\"Use matplotlib and meshgrid to visualize decision boundaries for different k values.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21abe85b",
      "metadata": {
        "id": "21abe85b"
      },
      "source": [
        "**Q25. Apply Feature Scaling before training a KNN model and compare results with unscaled data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6354b548",
      "metadata": {
        "id": "6354b548"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train_s, X_test_s, y_train, y_test = train_test_split(X_scaled, y)\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train_s, y_train)\n",
        "print(\"Scaled Accuracy:\", model.score(X_test_s, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10cfec32",
      "metadata": {
        "id": "10cfec32"
      },
      "source": [
        "**Q26. Train a PCA model on synthetic data and print the explained variance ratio for each component**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ca6451",
      "metadata": {
        "id": "e9ca6451"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X)\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78386756",
      "metadata": {
        "id": "78386756"
      },
      "source": [
        "**Q27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f651403e",
      "metadata": {
        "id": "f651403e"
      },
      "outputs": [],
      "source": [
        "X_pca = PCA(n_components=2).fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y)\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"PCA+KNN Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df632059",
      "metadata": {
        "id": "df632059"
      },
      "source": [
        "**Q28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68eaef33",
      "metadata": {
        "id": "68eaef33"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'n_neighbors': [3, 5, 7]}\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best K:\", grid.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74686042",
      "metadata": {
        "id": "74686042"
      },
      "source": [
        "**Q29. Train a KNN Classifier and check the number of misclassified samples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c682f5",
      "metadata": {
        "id": "b8c682f5"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "misclassified = (y_pred != y_test).sum()\n",
        "print(\"Misclassified samples:\", misclassified)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878fffb3",
      "metadata": {
        "id": "878fffb3"
      },
      "source": [
        "**Q30. Train a PCA model and visualize the cumulative explained variance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36147fcf",
      "metadata": {
        "id": "36147fcf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "plt.plot(cumsum)\n",
        "plt.title(\"Cumulative Explained Variance\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "669de908",
      "metadata": {
        "id": "669de908"
      },
      "source": [
        "**Q31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5de46ab",
      "metadata": {
        "id": "a5de46ab"
      },
      "outputs": [],
      "source": [
        "for w in ['uniform', 'distance']:\n",
        "    model = KNeighborsClassifier(weights=w)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Weight={w}, Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3419ec",
      "metadata": {
        "id": "3d3419ec"
      },
      "source": [
        "**Q32. Train a KNN Regressor and analyze the effect of different K values on performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "594b5ee7",
      "metadata": {
        "id": "594b5ee7"
      },
      "outputs": [],
      "source": [
        "for k in [1, 3, 5, 10]:\n",
        "    model = KNeighborsRegressor(n_neighbors=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"K={k}, MSE:\", mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d43c2c0",
      "metadata": {
        "id": "8d43c2c0"
      },
      "source": [
        "**Q33. Implement KNN Imputation for handling missing values in a dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22975169",
      "metadata": {
        "id": "22975169"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "X_missing = X.copy()\n",
        "X_missing[::10] = np.nan\n",
        "imputer = KNNImputer()\n",
        "X_filled = imputer.fit_transform(X_missing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2383972",
      "metadata": {
        "id": "c2383972"
      },
      "source": [
        "**Q34. Train a PCA model and visualize the data projection onto the first two principal components**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4aeefc3",
      "metadata": {
        "id": "e4aeefc3"
      },
      "outputs": [],
      "source": [
        "X_pca = PCA(n_components=2).fit_transform(X_scaled)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "plt.title(\"PCA Projection\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0c453a",
      "metadata": {
        "id": "fe0c453a"
      },
      "source": [
        "**Q35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed4d0eb2",
      "metadata": {
        "id": "ed4d0eb2"
      },
      "outputs": [],
      "source": [
        "for algorithm in ['kd_tree', 'ball_tree']:\n",
        "    model = KNeighborsClassifier(algorithm=algorithm)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"{algorithm} Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379bf76d",
      "metadata": {
        "id": "379bf76d"
      },
      "source": [
        "**Q36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2735fa6f",
      "metadata": {
        "id": "2735fa6f"
      },
      "outputs": [],
      "source": [
        "plt.bar(range(len(pca.explained_variance_)), pca.explained_variance_)\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Component\")\n",
        "plt.ylabel(\"Variance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7031a101",
      "metadata": {
        "id": "7031a101"
      },
      "source": [
        "**Q37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a62a8b6",
      "metadata": {
        "id": "3a62a8b6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cae585a",
      "metadata": {
        "id": "2cae585a"
      },
      "source": [
        "**Q38. Train a PCA model and analyze the effect of different numbers of components on accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85953c8a",
      "metadata": {
        "id": "85953c8a"
      },
      "outputs": [],
      "source": [
        "for n in [1, 2, 3]:\n",
        "    X_pca = PCA(n_components=n).fit_transform(X_scaled)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"n={n}, Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "851341b6",
      "metadata": {
        "id": "851341b6"
      },
      "source": [
        "**Q39. Train a KNN Classifier with different leaf_size values and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9aa52b6",
      "metadata": {
        "id": "b9aa52b6"
      },
      "outputs": [],
      "source": [
        "for leaf_size in [10, 30, 50]:\n",
        "    model = KNeighborsClassifier(leaf_size=leaf_size)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Leaf Size={leaf_size}, Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be684544",
      "metadata": {
        "id": "be684544"
      },
      "source": [
        "**Q40. Train a PCA model and visualize how data points are transformed before and after PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4e548d6",
      "metadata": {
        "id": "f4e548d6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.title(\"Original Data\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "plt.title(\"After PCA\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6bb142",
      "metadata": {
        "id": "df6bb142"
      },
      "source": [
        "**Q41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "517abd83",
      "metadata": {
        "id": "517abd83"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d4927a",
      "metadata": {
        "id": "88d4927a"
      },
      "source": [
        "**Q42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f4a3f0c",
      "metadata": {
        "id": "2f4a3f0c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "for metric in ['euclidean', 'manhattan']:\n",
        "    model = KNeighborsRegressor(metric=metric)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"{metric.title()} MAE:\", mean_absolute_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f46521bb",
      "metadata": {
        "id": "f46521bb"
      },
      "source": [
        "**Q43. Train a KNN Classifier and evaluate using ROC-AUC score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c19f7820",
      "metadata": {
        "id": "c19f7820"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bin)\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_score = model.predict_proba(X_test)\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_score, multi_class='ovr'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7e9c47",
      "metadata": {
        "id": "fd7e9c47"
      },
      "source": [
        "**Q44. Train a PCA model and visualize the variance captured by each principal component**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53104e9",
      "metadata": {
        "id": "a53104e9"
      },
      "outputs": [],
      "source": [
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         pca.explained_variance_ratio_, marker='o')\n",
        "plt.title(\"Variance per Principal Component\")\n",
        "plt.xlabel(\"Component\")\n",
        "plt.ylabel(\"Variance Ratio\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce4d80d",
      "metadata": {
        "id": "7ce4d80d"
      },
      "source": [
        "**Q45. Train a KNN Classifier and perform feature selection before training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aca3ecf",
      "metadata": {
        "id": "1aca3ecf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y)\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy with Feature Selection:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55549a4d",
      "metadata": {
        "id": "55549a4d"
      },
      "source": [
        "**Q46. Train a PCA model and visualize the data reconstruction error after reducing dimensions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0740cf3b",
      "metadata": {
        "id": "0740cf3b"
      },
      "outputs": [],
      "source": [
        "X_pca = PCA(n_components=2).fit_transform(X_scaled)\n",
        "X_reconstructed = PCA(n_components=2).fit(X_scaled).inverse_transform(X_pca)\n",
        "reconstruction_error = ((X_scaled - X_reconstructed) ** 2).mean()\n",
        "print(\"Reconstruction Error:\", reconstruction_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf47f9e4",
      "metadata": {
        "id": "bf47f9e4"
      },
      "source": [
        "**Q47. Train a KNN Classifier and visualize the decision boundary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a10d292",
      "metadata": {
        "id": "5a10d292"
      },
      "outputs": [],
      "source": [
        "# Decision boundaries need matplotlib for 2D data\n",
        "print(\"Use matplotlib to visualize decision boundary for 2D KNN classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9336027",
      "metadata": {
        "id": "f9336027"
      },
      "source": [
        "**Q48. Train a PCA model and analyze the effect of different numbers of components on data variance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3457edfc",
      "metadata": {
        "id": "3457edfc"
      },
      "outputs": [],
      "source": [
        "explained = []\n",
        "for n in range(1, X.shape[1] + 1):\n",
        "    pca = PCA(n_components=n)\n",
        "    pca.fit(X)\n",
        "    explained.append(np.sum(pca.explained_variance_ratio_))\n",
        "\n",
        "plt.plot(range(1, len(explained) + 1), explained, marker='o')\n",
        "plt.title(\"Variance vs Components\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}