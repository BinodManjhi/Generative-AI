{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54a26e8b",
      "metadata": {
        "id": "54a26e8b"
      },
      "source": [
        "# **Ensemble Learning Assignment**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f03e1a00",
      "metadata": {
        "id": "f03e1a00"
      },
      "source": [
        "## ðŸ§  Theoretical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a88dcd45",
      "metadata": {
        "id": "a88dcd45"
      },
      "source": [
        "**Q1. Can we use Bagging for regression problems**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aec9c90",
      "metadata": {
        "id": "4aec9c90"
      },
      "source": [
        "Yes, Bagging can be used for regression tasks. The BaggingRegressor in sklearn is specifically designed for such problems by averaging predictions of multiple base regressors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e66da9",
      "metadata": {
        "id": "f3e66da9"
      },
      "source": [
        "**Q2. What is the difference between multiple model training and single model training**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e47d9e3",
      "metadata": {
        "id": "0e47d9e3"
      },
      "source": [
        "Single model training involves learning one model from the data. Multiple model training (ensemble) combines predictions from several models to improve generalization and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0afde02b",
      "metadata": {
        "id": "0afde02b"
      },
      "source": [
        "**Q3. Explain the concept of feature randomness in Random Forest**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18694c56",
      "metadata": {
        "id": "18694c56"
      },
      "source": [
        "In Random Forest, feature randomness means each tree considers a random subset of features while splitting, which helps in reducing correlation among trees and improves model robustness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41eb34e9",
      "metadata": {
        "id": "41eb34e9"
      },
      "source": [
        "**Q4. What is OOB (Out-of-Bag) Score**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d61f519f",
      "metadata": {
        "id": "d61f519f"
      },
      "source": [
        "Out-of-Bag (OOB) Score is an internal validation method in Bagging/Random Forest. It uses data not included in a particular bootstrap sample to evaluate model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2defce",
      "metadata": {
        "id": "ad2defce"
      },
      "source": [
        "**Q5. How can you measure the importance of features in a Random Forest model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697e3341",
      "metadata": {
        "id": "697e3341"
      },
      "source": [
        "Feature importance in Random Forest is measured by how much each feature decreases impurity across all trees. sklearn provides `.feature_importances_` for this purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d624b68",
      "metadata": {
        "id": "9d624b68"
      },
      "source": [
        "**Q6. Explain the working principle of a Bagging Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56257e8",
      "metadata": {
        "id": "a56257e8"
      },
      "source": [
        "Bagging Classifier builds multiple models (like Decision Trees) using different bootstrap samples and combines their predictions by majority voting (classification) or averaging (regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2ea946",
      "metadata": {
        "id": "6f2ea946"
      },
      "source": [
        "**Q7. How do you evaluate a Bagging Classifierâ€™s performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391564f2",
      "metadata": {
        "id": "391564f2"
      },
      "source": [
        "A Bagging Classifierâ€™s performance is evaluated using standard metrics like accuracy, precision, recall, F1-score, etc., often compared to base estimators."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83fda9a5",
      "metadata": {
        "id": "83fda9a5"
      },
      "source": [
        "**Q8. How does a Bagging Regressor work**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa2e5d5",
      "metadata": {
        "id": "ffa2e5d5"
      },
      "source": [
        "A Bagging Regressor works by creating multiple base regressors trained on bootstrap samples and aggregating their predictions via averaging to reduce variance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b82f0e3",
      "metadata": {
        "id": "6b82f0e3"
      },
      "source": [
        "**Q9. What is the main advantage of ensemble techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1edde001",
      "metadata": {
        "id": "1edde001"
      },
      "source": [
        "Ensemble techniques reduce overfitting and variance, leading to improved predictive performance compared to individual models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f27551",
      "metadata": {
        "id": "69f27551"
      },
      "source": [
        "**Q10. What is the main challenge of ensemble methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b61f9de",
      "metadata": {
        "id": "1b61f9de"
      },
      "source": [
        "The main challenge of ensemble methods is increased computational complexity and reduced interpretability of the final model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5011ddbf",
      "metadata": {
        "id": "5011ddbf"
      },
      "source": [
        "**Q11. Explain the key idea behind ensemble techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3012772",
      "metadata": {
        "id": "f3012772"
      },
      "source": [
        "Ensemble techniques combine the strengths of multiple models to produce a robust predictor that often outperforms individual models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b5134d",
      "metadata": {
        "id": "51b5134d"
      },
      "source": [
        "**Q12. What is a Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b6149e",
      "metadata": {
        "id": "86b6149e"
      },
      "source": [
        "A Random Forest Classifier is an ensemble of decision trees where each tree votes and the majority vote is the final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bcc2107",
      "metadata": {
        "id": "5bcc2107"
      },
      "source": [
        "**Q13. What are the main types of ensemble techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "463a5795",
      "metadata": {
        "id": "463a5795"
      },
      "source": [
        "The main types of ensemble techniques are Bagging, Boosting, and Stacking."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c6e02b",
      "metadata": {
        "id": "62c6e02b"
      },
      "source": [
        "**Q14. What is ensemble learning in machine learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a02d45c",
      "metadata": {
        "id": "6a02d45c"
      },
      "source": [
        "Ensemble learning is a method that combines predictions from multiple models to improve accuracy and robustness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e30a00dc",
      "metadata": {
        "id": "e30a00dc"
      },
      "source": [
        "**Q15. When should we avoid using ensemble methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2896b4ff",
      "metadata": {
        "id": "2896b4ff"
      },
      "source": [
        "We should avoid ensemble methods when interpretability is important or the computational cost outweighs performance gain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bcc6df2",
      "metadata": {
        "id": "7bcc6df2"
      },
      "source": [
        "**Q16. How does Bagging help in reducing overfitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c57a7e7",
      "metadata": {
        "id": "2c57a7e7"
      },
      "source": [
        "Bagging reduces overfitting by averaging multiple models trained on random subsets of data, thus reducing variance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1cfde19",
      "metadata": {
        "id": "b1cfde19"
      },
      "source": [
        "**Q17. Why is Random Forest better than a single Decision Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2529114",
      "metadata": {
        "id": "a2529114"
      },
      "source": [
        "Random Forest aggregates multiple trees, reducing overfitting of a single Decision Tree and increasing generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d34d8dd",
      "metadata": {
        "id": "4d34d8dd"
      },
      "source": [
        "**Q18. What is the role of bootstrap sampling in Bagging**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2ef2374",
      "metadata": {
        "id": "c2ef2374"
      },
      "source": [
        "Bootstrap sampling allows training each model on a slightly different dataset, promoting diversity in predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d425b10e",
      "metadata": {
        "id": "d425b10e"
      },
      "source": [
        "**Q19. What are some real-world applications of ensemble techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842bc3aa",
      "metadata": {
        "id": "842bc3aa"
      },
      "source": [
        "Applications include fraud detection, recommendation systems, credit scoring, and medical diagnosis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef7b36b0",
      "metadata": {
        "id": "ef7b36b0"
      },
      "source": [
        "**Q20. What is the difference between Bagging and Boosting?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3437aa8",
      "metadata": {
        "id": "b3437aa8"
      },
      "source": [
        "Bagging reduces variance by averaging models; Boosting reduces bias by sequentially training models to correct predecessors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c85aa6",
      "metadata": {
        "id": "74c85aa6"
      },
      "source": [
        "## ðŸ§ª Practical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7a7decc",
      "metadata": {
        "id": "d7a7decc"
      },
      "source": [
        "**Q21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6abb8a9",
      "metadata": {
        "id": "f6abb8a9"
      },
      "outputs": [],
      "source": [
        "# Q21: Bagging Classifier using Decision Trees\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a8b0890",
      "metadata": {
        "id": "3a8b0890"
      },
      "source": [
        "**Q22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7463a42c",
      "metadata": {
        "id": "7463a42c"
      },
      "outputs": [],
      "source": [
        "# Q22: Bagging Regressor using Decision Trees\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"MSE:\", mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d476ed",
      "metadata": {
        "id": "66d476ed"
      },
      "source": [
        "**Q23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57338f6c",
      "metadata": {
        "id": "57338f6c"
      },
      "outputs": [],
      "source": [
        "# Q23: Random Forest Classifier on Breast Cancer dataset\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "importances = model.feature_importances_\n",
        "print(\"Feature importances:\", importances)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e20107",
      "metadata": {
        "id": "64e20107"
      },
      "source": [
        "**Q24. Train a Random Forest Regressor and compare its performance with a single Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e181b6",
      "metadata": {
        "id": "f6e181b6"
      },
      "outputs": [],
      "source": [
        "# Q24: Compare RF Regressor and Decision Tree\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "dt = DecisionTreeRegressor()\n",
        "rf = RandomForestRegressor()\n",
        "dt.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "print(\"Decision Tree Score:\", dt.score(X_test, y_test))\n",
        "print(\"Random Forest Score:\", rf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5c9708f",
      "metadata": {
        "id": "b5c9708f"
      },
      "source": [
        "**Q25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74fea51e",
      "metadata": {
        "id": "74fea51e"
      },
      "outputs": [],
      "source": [
        "# Q25: Compute OOB Score\n",
        "model = RandomForestClassifier(oob_score=True)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"OOB Score:\", model.oob_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1389d34d",
      "metadata": {
        "id": "1389d34d"
      },
      "source": [
        "**Q26. Train a Bagging Classifier using SVM as a base estimator and print accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b0223d",
      "metadata": {
        "id": "25b0223d"
      },
      "outputs": [],
      "source": [
        "# Q26: Bagging Classifier with SVM\n",
        "from sklearn.svm import SVC\n",
        "model = BaggingClassifier(base_estimator=SVC(), n_estimators=10)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67e3965e",
      "metadata": {
        "id": "67e3965e"
      },
      "source": [
        "**Q27. Train a Random Forest Classifier with different numbers of trees and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93308095",
      "metadata": {
        "id": "93308095"
      },
      "outputs": [],
      "source": [
        "# Q27: Random Forest Classifier with different n_estimators\n",
        "for n in [10, 50, 100]:\n",
        "    rf = RandomForestClassifier(n_estimators=n)\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(f\"{n} Trees Accuracy:\", rf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "676841f1",
      "metadata": {
        "id": "676841f1"
      },
      "source": [
        "**Q28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c3d3a68",
      "metadata": {
        "id": "6c3d3a68"
      },
      "outputs": [],
      "source": [
        "# Q28: Bagging with Logistic Regression and AUC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "model = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=10)\n",
        "model.fit(X_train, y_train)\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c966122",
      "metadata": {
        "id": "0c966122"
      },
      "source": [
        "**Q29. Train a Random Forest Regressor and analyze feature importance scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39fb68e7",
      "metadata": {
        "id": "39fb68e7"
      },
      "outputs": [],
      "source": [
        "# Q29: RF Regressor feature importance\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Top Features:\", model.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f395413b",
      "metadata": {
        "id": "f395413b"
      },
      "source": [
        "**Q30. Train an ensemble model using both Bagging and Random Forest and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4263c6ea",
      "metadata": {
        "id": "4263c6ea"
      },
      "outputs": [],
      "source": [
        "# Q30: Compare Bagging and RF accuracy\n",
        "bag = BaggingClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "bag.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "print(\"Bagging Accuracy:\", bag.score(X_test, y_test))\n",
        "print(\"RF Accuracy:\", rf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "537130f1",
      "metadata": {
        "id": "537130f1"
      },
      "source": [
        "**Q31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1cee129",
      "metadata": {
        "id": "f1cee129"
      },
      "outputs": [],
      "source": [
        "# Q31: GridSearchCV for RF\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'n_estimators': [50, 100], 'max_depth': [None, 10]}\n",
        "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b49052",
      "metadata": {
        "id": "d8b49052"
      },
      "source": [
        "**Q32. Train a Bagging Regressor with different numbers of base estimators and compare performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75abafab",
      "metadata": {
        "id": "75abafab"
      },
      "outputs": [],
      "source": [
        "# Q32: Bagging Regressor with different estimators\n",
        "for n in [5, 10, 20]:\n",
        "    model = BaggingRegressor(n_estimators=n)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"n_estimators={n}, Score:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3887f016",
      "metadata": {
        "id": "3887f016"
      },
      "source": [
        "**Q33. Train a Random Forest Classifier and analyze misclassified samples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "747f0e73",
      "metadata": {
        "id": "747f0e73"
      },
      "outputs": [],
      "source": [
        "# Q33: Analyze misclassified samples\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "misclassified = (preds != y_test)\n",
        "print(\"Misclassified Samples:\", sum(misclassified))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea99198",
      "metadata": {
        "id": "5ea99198"
      },
      "source": [
        "**Q34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "749831a0",
      "metadata": {
        "id": "749831a0"
      },
      "outputs": [],
      "source": [
        "# Q34: Bagging vs Single Decision Tree\n",
        "dt = DecisionTreeClassifier()\n",
        "bag = BaggingClassifier(base_estimator=dt)\n",
        "dt.fit(X_train, y_train)\n",
        "bag.fit(X_train, y_train)\n",
        "print(\"Decision Tree Accuracy:\", dt.score(X_test, y_test))\n",
        "print(\"Bagging Accuracy:\", bag.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18edfa49",
      "metadata": {
        "id": "18edfa49"
      },
      "source": [
        "**Q35. Train a Random Forest Classifier and visualize the confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f92890",
      "metadata": {
        "id": "e7f92890"
      },
      "outputs": [],
      "source": [
        "# Q35: RF Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "cm = confusion_matrix(y_test, model.predict(X_test))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f4471a",
      "metadata": {
        "id": "a4f4471a"
      },
      "source": [
        "**Q36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "244f1f8b",
      "metadata": {
        "id": "244f1f8b"
      },
      "outputs": [],
      "source": [
        "# Q36: Stacking Classifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "base_learners = [('dt', DecisionTreeClassifier()), ('svm', SVC(probability=True))]\n",
        "stack = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Accuracy:\", stack.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f8abfc",
      "metadata": {
        "id": "b6f8abfc"
      },
      "source": [
        "**Q37. Train a Random Forest Classifier and print the top 5 most important features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93517b6",
      "metadata": {
        "id": "f93517b6"
      },
      "outputs": [],
      "source": [
        "# Q37: Top 5 Features from RF\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "import numpy as np\n",
        "top_features = np.argsort(model.feature_importances_)[-5:]\n",
        "print(\"Top 5 Feature Indices:\", top_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8685b7fb",
      "metadata": {
        "id": "8685b7fb"
      },
      "source": [
        "**Q38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4f42a1",
      "metadata": {
        "id": "7f4f42a1"
      },
      "outputs": [],
      "source": [
        "# Q38: Bagging with precision, recall, F1\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "model = BaggingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "743e4c9d",
      "metadata": {
        "id": "743e4c9d"
      },
      "source": [
        "**Q39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b51ff9",
      "metadata": {
        "id": "e4b51ff9"
      },
      "outputs": [],
      "source": [
        "# Q39: RF accuracy with different max_depth\n",
        "for depth in [5, 10, None]:\n",
        "    rf = RandomForestClassifier(max_depth=depth)\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(f\"max_depth={depth}, Accuracy:\", rf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cd92b0",
      "metadata": {
        "id": "f6cd92b0"
      },
      "source": [
        "**Q40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6752da84",
      "metadata": {
        "id": "6752da84"
      },
      "outputs": [],
      "source": [
        "# Q40: Bagging Regressor with DT & KNN\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "for base in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
        "    model = BaggingRegressor(base_estimator=base, n_estimators=10)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"{type(base).__name__} Score:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c2c36d8",
      "metadata": {
        "id": "3c2c36d8"
      },
      "source": [
        "**Q41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1511c38",
      "metadata": {
        "id": "b1511c38"
      },
      "outputs": [],
      "source": [
        "# Q41: RF ROC-AUC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae8b499",
      "metadata": {
        "id": "5ae8b499"
      },
      "source": [
        "**Q42. Train a Bagging Classifier and evaluate its performance using cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314411f1",
      "metadata": {
        "id": "314411f1"
      },
      "outputs": [],
      "source": [
        "# Q42: Cross-validation for Bagging Classifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(BaggingClassifier(), X, y, cv=5)\n",
        "print(\"CV Accuracy:\", scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e356b37b",
      "metadata": {
        "id": "e356b37b"
      },
      "source": [
        "**Q43. Train a Random Forest Classifier and plot the Precision-Recall curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86989ec4",
      "metadata": {
        "id": "86989ec4"
      },
      "outputs": [],
      "source": [
        "# Q43: RF Precision-Recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, probs)\n",
        "plt.plot(recall, precision)\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b66a4bf",
      "metadata": {
        "id": "6b66a4bf"
      },
      "source": [
        "**Q44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed613691",
      "metadata": {
        "id": "ed613691"
      },
      "outputs": [],
      "source": [
        "# Q44: Stacking with RF & LR\n",
        "base_learners = [('rf', RandomForestClassifier())]\n",
        "stack = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacked Accuracy:\", stack.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "297e0864",
      "metadata": {
        "id": "297e0864"
      },
      "source": [
        "**Q45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41dea207",
      "metadata": {
        "id": "41dea207"
      },
      "outputs": [],
      "source": [
        "# Q45: Bagging Regressor with bootstrap variation\n",
        "for b in [True, False]:\n",
        "    model = BaggingRegressor(bootstrap=b)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Bootstrap={b}, Score:\", model.score(X_test, y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}