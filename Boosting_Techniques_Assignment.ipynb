{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d303aa1e",
      "metadata": {
        "id": "d303aa1e"
      },
      "source": [
        "## ðŸ§  Theoretical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c10cde3",
      "metadata": {
        "id": "3c10cde3"
      },
      "source": [
        "**Q1. What is Boosting in Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c1d60c",
      "metadata": {
        "id": "38c1d60c"
      },
      "source": [
        "Boosting is an ensemble technique that combines multiple weak learners to form a strong learner by focusing more on the errors made by previous models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e346c00",
      "metadata": {
        "id": "3e346c00"
      },
      "source": [
        "**Q2. How does Boosting differ from Bagging**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a71f2759",
      "metadata": {
        "id": "a71f2759"
      },
      "source": [
        "Bagging builds models independently in parallel; Boosting builds models sequentially, each trying to correct the errors of the previous one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fefeaba7",
      "metadata": {
        "id": "fefeaba7"
      },
      "source": [
        "**Q3. What is the key idea behind AdaBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d2fdc40",
      "metadata": {
        "id": "3d2fdc40"
      },
      "source": [
        "AdaBoost adjusts the weights of observations based on previous classification performance and gives more focus to misclassified data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "918fc4e2",
      "metadata": {
        "id": "918fc4e2"
      },
      "source": [
        "**Q4. Explain the working of AdaBoost with an example**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af89d1a5",
      "metadata": {
        "id": "af89d1a5"
      },
      "source": [
        "AdaBoost trains a sequence of weak learners, updating weights of incorrectly classified instances and combining models using weighted majority vote."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "238d8494",
      "metadata": {
        "id": "238d8494"
      },
      "source": [
        "**Q5. What is Gradient Boosting, and how is it different from AdaBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de43a7a",
      "metadata": {
        "id": "6de43a7a"
      },
      "source": [
        "Gradient Boosting builds models sequentially like AdaBoost but uses gradient descent to minimize a loss function for better predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0207f7e5",
      "metadata": {
        "id": "0207f7e5"
      },
      "source": [
        "**Q6. What is the loss function in Gradient Boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c7c9b4a",
      "metadata": {
        "id": "1c7c9b4a"
      },
      "source": [
        "The loss function in Gradient Boosting is typically the squared error for regression or log loss for classification, minimized using gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9494c4",
      "metadata": {
        "id": "bc9494c4"
      },
      "source": [
        "**Q7. How does XGBoost improve over traditional Gradient Boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51243a32",
      "metadata": {
        "id": "51243a32"
      },
      "source": [
        "XGBoost enhances Gradient Boosting with regularization, tree pruning, parallel processing, and handling missing data efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f91569a5",
      "metadata": {
        "id": "f91569a5"
      },
      "source": [
        "**Q8. What is the difference between XGBoost and CatBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f5f6d75",
      "metadata": {
        "id": "1f5f6d75"
      },
      "source": [
        "XGBoost is faster and more tunable, while CatBoost handles categorical variables natively and avoids overfitting using ordered boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc91a70c",
      "metadata": {
        "id": "bc91a70c"
      },
      "source": [
        "**Q9. What are some real-world applications of Boosting techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9421bd0b",
      "metadata": {
        "id": "9421bd0b"
      },
      "source": [
        "Boosting is used in fraud detection, customer churn prediction, text classification, medical diagnostics, and recommendation systems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e94aea",
      "metadata": {
        "id": "e4e94aea"
      },
      "source": [
        "**Q10. How does regularization help in XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e947eb",
      "metadata": {
        "id": "b0e947eb"
      },
      "source": [
        "Regularization in XGBoost controls overfitting by adding penalty terms to the loss function for model complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b66cf798",
      "metadata": {
        "id": "b66cf798"
      },
      "source": [
        "**Q11. What are some hyperparameters to tune in Gradient Boosting models**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1554fa0",
      "metadata": {
        "id": "b1554fa0"
      },
      "source": [
        "Hyperparameters include learning rate, number of estimators, max depth, min samples split, subsample ratio, and regularization terms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b697e6",
      "metadata": {
        "id": "c8b697e6"
      },
      "source": [
        "**Q12. What is the concept of Feature Importance in Boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a251c51c",
      "metadata": {
        "id": "a251c51c"
      },
      "source": [
        "Feature importance in Boosting is calculated based on how frequently a feature is used in trees and its contribution to reducing error."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cdb75ab",
      "metadata": {
        "id": "8cdb75ab"
      },
      "source": [
        "**Q13. Why is CatBoost efficient for categorical data?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b908c8",
      "metadata": {
        "id": "50b908c8"
      },
      "source": [
        "CatBoost efficiently processes categorical data by converting them internally using techniques like ordered statistics and avoids target leakage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff091283",
      "metadata": {
        "id": "ff091283"
      },
      "source": [
        "## ðŸ§ª Practical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9837cfc4",
      "metadata": {
        "id": "9837cfc4"
      },
      "source": [
        "**Q14. Train an AdaBoost Classifier on a sample dataset and print model accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f67ad28",
      "metadata": {
        "id": "5f67ad28"
      },
      "outputs": [],
      "source": [
        "# Q14: AdaBoost Classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = AdaBoostClassifier(n_estimators=50)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc8c9c9",
      "metadata": {
        "id": "2dc8c9c9"
      },
      "source": [
        "**Q15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef90d6c",
      "metadata": {
        "id": "2ef90d6c"
      },
      "outputs": [],
      "source": [
        "# Q15: AdaBoost Regressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = AdaBoostRegressor(n_estimators=50)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"MAE:\", mean_absolute_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79712e47",
      "metadata": {
        "id": "79712e47"
      },
      "source": [
        "**Q16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4435d6b7",
      "metadata": {
        "id": "4435d6b7"
      },
      "outputs": [],
      "source": [
        "# Q16: Gradient Boosting Classifier Feature Importance\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Feature Importance:\", model.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afbc8651",
      "metadata": {
        "id": "afbc8651"
      },
      "source": [
        "**Q17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dfcc41e",
      "metadata": {
        "id": "9dfcc41e"
      },
      "outputs": [],
      "source": [
        "# Q17: Gradient Boosting Regressor R2 Score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"R2 Score:\", r2_score(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5942629f",
      "metadata": {
        "id": "5942629f"
      },
      "source": [
        "**Q18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b2ffec",
      "metadata": {
        "id": "32b2ffec"
      },
      "outputs": [],
      "source": [
        "# Q18: XGBoost Classifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c85934",
      "metadata": {
        "id": "a3c85934"
      },
      "source": [
        "**Q19. Train a CatBoost Classifier and evaluate using F1-Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b82be73d",
      "metadata": {
        "id": "b82be73d"
      },
      "outputs": [],
      "source": [
        "# Q19: CatBoost Classifier F1 Score\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da47f6d",
      "metadata": {
        "id": "6da47f6d"
      },
      "source": [
        "**Q20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2db01b5",
      "metadata": {
        "id": "e2db01b5"
      },
      "outputs": [],
      "source": [
        "# Q20: XGBoost Regressor MSE\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"MSE:\", mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee643fc7",
      "metadata": {
        "id": "ee643fc7"
      },
      "source": [
        "**Q21. Train an AdaBoost Classifier and visualize feature importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce0dbe51",
      "metadata": {
        "id": "ce0dbe51"
      },
      "outputs": [],
      "source": [
        "# Q21: AdaBoost Feature Importance Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = AdaBoostClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "importance = model.feature_importances_\n",
        "plt.bar(range(len(importance)), importance)\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f520d2",
      "metadata": {
        "id": "49f520d2"
      },
      "source": [
        "**Q22. Train a Gradient Boosting Regressor and plot learning curves**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8cbe2dc",
      "metadata": {
        "id": "e8cbe2dc"
      },
      "outputs": [],
      "source": [
        "# Q22: Gradient Boosting Regressor Learning Curves\n",
        "train_scores, test_scores = [], []\n",
        "for i in range(1, 51):\n",
        "    model = GradientBoostingRegressor(n_estimators=i)\n",
        "    model.fit(X_train, y_train)\n",
        "    train_scores.append(model.score(X_train, y_train))\n",
        "    test_scores.append(model.score(X_test, y_test))\n",
        "\n",
        "plt.plot(train_scores, label='Train')\n",
        "plt.plot(test_scores, label='Test')\n",
        "plt.legend()\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697eaafb",
      "metadata": {
        "id": "697eaafb"
      },
      "source": [
        "**Q23. Train an XGBoost Classifier and visualize feature importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d470f9da",
      "metadata": {
        "id": "d470f9da"
      },
      "outputs": [],
      "source": [
        "# Q23: XGBoost Feature Importance Plot\n",
        "from xgboost import plot_importance\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "plot_importance(model)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75584b01",
      "metadata": {
        "id": "75584b01"
      },
      "source": [
        "**Q24. Train a CatBoost Classifier and plot the confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b6fe1c2",
      "metadata": {
        "id": "8b6fe1c2"
      },
      "outputs": [],
      "source": [
        "# Q24: CatBoost Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "cm = confusion_matrix(y_test, model.predict(X_test))\n",
        "sns.heatmap(cm, annot=True)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d1b7b1",
      "metadata": {
        "id": "71d1b7b1"
      },
      "source": [
        "**Q25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51ca8902",
      "metadata": {
        "id": "51ca8902"
      },
      "outputs": [],
      "source": [
        "# Q25: AdaBoost varying estimators\n",
        "for n in [10, 50, 100]:\n",
        "    model = AdaBoostClassifier(n_estimators=n)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Estimators={n}, Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397109a1",
      "metadata": {
        "id": "397109a1"
      },
      "source": [
        "**Q26. Train a Gradient Boosting Classifier and visualize the ROC curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b28a29",
      "metadata": {
        "id": "c4b28a29"
      },
      "outputs": [],
      "source": [
        "# Q26: Gradient Boosting ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.xlabel(\"FPR\")\n",
        "plt.ylabel(\"TPR\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993e8d9e",
      "metadata": {
        "id": "993e8d9e"
      },
      "source": [
        "**Q27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e025ba",
      "metadata": {
        "id": "48e025ba"
      },
      "outputs": [],
      "source": [
        "# Q27: XGBoost GridSearchCV Learning Rate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid = GridSearchCV(XGBRegressor(), {'learning_rate': [0.01, 0.1, 0.2]}, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7d49c6",
      "metadata": {
        "id": "7f7d49c6"
      },
      "source": [
        "**Q28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6270a780",
      "metadata": {
        "id": "6270a780"
      },
      "outputs": [],
      "source": [
        "# Q28: CatBoost on Imbalanced Dataset\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "weights = class_weight.compute_sample_weight('balanced', y_train)\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train, sample_weight=weights)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6ba0b21",
      "metadata": {
        "id": "b6ba0b21"
      },
      "source": [
        "**Q29. Train an AdaBoost Classifier and analyze the effect of different learning rates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b276e9b",
      "metadata": {
        "id": "0b276e9b"
      },
      "outputs": [],
      "source": [
        "# Q29: AdaBoost - Effect of Learning Rate\n",
        "for lr in [0.01, 0.1, 1.0]:\n",
        "    model = AdaBoostClassifier(learning_rate=lr)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Learning Rate={lr}, Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4176fa26",
      "metadata": {
        "id": "4176fa26"
      },
      "source": [
        "**Q30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4911734",
      "metadata": {
        "id": "e4911734"
      },
      "outputs": [],
      "source": [
        "# Q30: XGBoost Multi-Class Classification\n",
        "model = XGBClassifier(objective='multi:softprob', num_class=3)\n",
        "model.fit(X_train, y_train)\n",
        "from sklearn.metrics import log_loss\n",
        "print(\"Log Loss:\", log_loss(y_test, model.predict_proba(X_test)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}