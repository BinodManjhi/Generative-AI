{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f08fd583",
      "metadata": {
        "id": "f08fd583"
      },
      "source": [
        "**1. What is unsupervised learning in the context of machine learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45c444d",
      "metadata": {
        "id": "d45c444d"
      },
      "source": [
        "**Answer:** Unsupervised learning is a type of machine learning where the model is not provided with labeled data. Instead, it tries to identify hidden patterns or intrinsic structures in the input data. Clustering is a common technique used in unsupervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e889bdd",
      "metadata": {
        "id": "0e889bdd"
      },
      "source": [
        "**2. How does K-Means clustering algorithm work**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69fdbe1c",
      "metadata": {
        "id": "69fdbe1c"
      },
      "source": [
        "**Answer:** K-Means works by initializing 'k' centroids, assigning each data point to the nearest centroid, and then updating the centroids based on the average of the assigned points. This process repeats until convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17357eb6",
      "metadata": {
        "id": "17357eb6"
      },
      "source": [
        "**3. Explain the concept of a dendrogram in hierarchical clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ad35b9",
      "metadata": {
        "id": "01ad35b9"
      },
      "source": [
        "**Answer:** A dendrogram is a tree-like diagram that records the sequences of merges or splits in hierarchical clustering. It helps visualize the arrangement of the clusters formed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b8eec4",
      "metadata": {
        "id": "00b8eec4"
      },
      "source": [
        "**4. What is the main difference between K-Means and Hierarchical Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5db5987",
      "metadata": {
        "id": "e5db5987"
      },
      "source": [
        "**Answer:** K-Means requires a predefined number of clusters and partitions the data into flat clusters, while Hierarchical Clustering builds a multilevel hierarchy of clusters without needing the number of clusters in advance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f94edba",
      "metadata": {
        "id": "9f94edba"
      },
      "source": [
        "**5. What are the advantages of DBSCAN over K-Means**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5d2fb7",
      "metadata": {
        "id": "ff5d2fb7"
      },
      "source": [
        "**Answer:** DBSCAN can find clusters of arbitrary shape and does not require the number of clusters to be specified. It can also identify noise points effectively, unlike K-Means."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df345ea3",
      "metadata": {
        "id": "df345ea3"
      },
      "source": [
        "**6. When would you use Silhouette Score in clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef64fc53",
      "metadata": {
        "id": "ef64fc53"
      },
      "source": [
        "**Answer:** Silhouette Score measures how similar a point is to its own cluster compared to other clusters. It is used to evaluate clustering performance without ground truth labels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a389f13",
      "metadata": {
        "id": "9a389f13"
      },
      "source": [
        "**7. What are the limitations of Hierarchical Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "773ecd1d",
      "metadata": {
        "id": "773ecd1d"
      },
      "source": [
        "**Answer:** Hierarchical Clustering can be computationally expensive for large datasets and is sensitive to noise and outliers. It is also not very flexible once the dendrogram is created."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d591479a",
      "metadata": {
        "id": "d591479a"
      },
      "source": [
        "**8. Why is feature scaling important in clustering algorithms like K-Means**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c228cd",
      "metadata": {
        "id": "21c228cd"
      },
      "source": [
        "**Answer:** Feature scaling ensures that all features contribute equally to the distance calculations used in clustering algorithms like K-Means. Without scaling, features with larger ranges can dominate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf42a503",
      "metadata": {
        "id": "bf42a503"
      },
      "source": [
        "**9. How does DBSCAN identify noise points**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c484b245",
      "metadata": {
        "id": "c484b245"
      },
      "source": [
        "**Answer:** DBSCAN identifies noise points as those that are not part of any dense region, meaning they do not have enough neighboring points within a defined distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e7e6bf",
      "metadata": {
        "id": "a9e7e6bf"
      },
      "source": [
        "**10. Define inertia in the context of K-Means**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ade286",
      "metadata": {
        "id": "e9ade286"
      },
      "source": [
        "**Answer:** Inertia is the sum of squared distances between each point and its assigned centroid in K-Means. Lower inertia indicates better clustering performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "523ea960",
      "metadata": {
        "id": "523ea960"
      },
      "source": [
        "**11. What is the elbow method in K-Means clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae29ea8",
      "metadata": {
        "id": "6ae29ea8"
      },
      "source": [
        "**Answer:** The elbow method plots inertia against the number of clusters to find the optimal 'k'. The point where the rate of decrease sharply changes (the elbow) is considered optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf10cbbe",
      "metadata": {
        "id": "cf10cbbe"
      },
      "source": [
        "**12. Describe the concept of \"density\" in DBSCAN**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d86580",
      "metadata": {
        "id": "c2d86580"
      },
      "source": [
        "**Answer:** In DBSCAN, density refers to the number of points within a specified radius. Dense regions are considered clusters, and sparse regions are treated as noise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d2a5d15",
      "metadata": {
        "id": "4d2a5d15"
      },
      "source": [
        "**13. Can hierarchical clustering be used on categorical data**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f683bd34",
      "metadata": {
        "id": "f683bd34"
      },
      "source": [
        "**Answer:** Yes, but it requires an appropriate similarity measure like Hamming distance since traditional hierarchical clustering relies on Euclidean distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3dd73ce",
      "metadata": {
        "id": "e3dd73ce"
      },
      "source": [
        "**14. What does a negative Silhouette Score indicate**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425a95f0",
      "metadata": {
        "id": "425a95f0"
      },
      "source": [
        "**Answer:** A negative Silhouette Score indicates that the sample is closer to a neighboring cluster than to the cluster it is assigned to, suggesting poor clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0203f255",
      "metadata": {
        "id": "0203f255"
      },
      "source": [
        "**15. Explain the term \"linkage criteria\" in hierarchical clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9a3de7",
      "metadata": {
        "id": "fb9a3de7"
      },
      "source": [
        "**Answer:** Linkage criteria determine how the distance between clusters is calculated in hierarchical clustering (e.g., single, complete, average linkage)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da7a629e",
      "metadata": {
        "id": "da7a629e"
      },
      "source": [
        "**16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dcdc48d",
      "metadata": {
        "id": "2dcdc48d"
      },
      "source": [
        "**Answer:** K-Means assumes clusters are spherical and similar in size. It struggles with clusters of different sizes, densities, or non-globular shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de20c121",
      "metadata": {
        "id": "de20c121"
      },
      "source": [
        "**17. What are the core parameters in DBSCAN, and how do they influence clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed3e0d8",
      "metadata": {
        "id": "bed3e0d8"
      },
      "source": [
        "**Answer:** The main parameters in DBSCAN are `eps` (the radius of the neighborhood) and `min_samples` (minimum number of points to form a dense region). These influence cluster formation and noise detection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853d5f63",
      "metadata": {
        "id": "853d5f63"
      },
      "source": [
        "**18. How does K-Means++ improve upon standard K-Means initialization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee3bf88c",
      "metadata": {
        "id": "ee3bf88c"
      },
      "source": [
        "**Answer:** K-Means++ improves K-Means by selecting initial centroids in a smarter way that spreads them out, leading to better and faster convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bffc8005",
      "metadata": {
        "id": "bffc8005"
      },
      "source": [
        "**19. What is agglomerative clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e750a2",
      "metadata": {
        "id": "39e750a2"
      },
      "source": [
        "**Answer:** Agglomerative clustering is a type of hierarchical clustering that starts with each point as its own cluster and merges the closest pairs of clusters iteratively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451c646f",
      "metadata": {
        "id": "451c646f"
      },
      "source": [
        "**20. What makes Silhouette Score a better metric than just inertia for model evaluation?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303bda6c",
      "metadata": {
        "id": "303bda6c"
      },
      "source": [
        "**Answer:** Silhouette Score considers intra-cluster cohesion and inter-cluster separation, offering a more comprehensive evaluation than inertia alone."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa5c46f",
      "metadata": {
        "id": "5aa5c46f"
      },
      "source": [
        "**Practical Questions:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1b6359",
      "metadata": {
        "id": "2a1b6359"
      },
      "source": [
        "**1. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7e23b7",
      "metadata": {
        "id": "5e7e23b7"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red')\n",
        "plt.title('KMeans Clustering with 4 Centers')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5b455d5",
      "metadata": {
        "id": "c5b455d5"
      },
      "source": [
        "**2. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af71e451",
      "metadata": {
        "id": "af71e451"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "print(\"First 10 predicted labels:\", labels[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e990e962",
      "metadata": {
        "id": "e990e962"
      },
      "source": [
        "**3. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "826977bf",
      "metadata": {
        "id": "826977bf"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_moons(n_samples=300, noise=0.1)\n",
        "db = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=db.labels_, cmap='Paired')\n",
        "plt.title('DBSCAN on make_moons data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa17e5fd",
      "metadata": {
        "id": "fa17e5fd"
      },
      "source": [
        "**4. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79b2ebb",
      "metadata": {
        "id": "d79b2ebb"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = load_wine()\n",
        "X = StandardScaler().fit_transform(data.data)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
        "labels, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "print(\"Cluster sizes:\", dict(zip(labels, counts)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48218f04",
      "metadata": {
        "id": "48218f04"
      },
      "source": [
        "**5. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2021c6dc",
      "metadata": {
        "id": "2021c6dc"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "db = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=db.labels_, cmap='plasma')\n",
        "plt.title('DBSCAN on make_circles data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39fd5c7f",
      "metadata": {
        "id": "39fd5c7f"
      },
      "source": [
        "**6. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6da2953",
      "metadata": {
        "id": "a6da2953"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = MinMaxScaler().fit_transform(data.data)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
        "print(\"Cluster centroids:\", kmeans.cluster_centers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aacb0e8",
      "metadata": {
        "id": "6aacb0e8"
      },
      "source": [
        "**7. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316a7f39",
      "metadata": {
        "id": "316a7f39"
      },
      "outputs": [],
      "source": [
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=0)\n",
        "db = DBSCAN(eps=0.9, min_samples=5).fit(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=db.labels_, cmap='tab10')\n",
        "plt.title('DBSCAN with varying cluster std')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746d714c",
      "metadata": {
        "id": "746d714c"
      },
      "source": [
        "**8. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce7475b",
      "metadata": {
        "id": "8ce7475b"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "digits = load_digits()\n",
        "X_pca = PCA(n_components=2).fit_transform(digits.data)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=0).fit(X_pca)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='tab10')\n",
        "plt.title('KMeans on Digits PCA')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae3abfe",
      "metadata": {
        "id": "3ae3abfe"
      },
      "source": [
        "**9. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dabd9468",
      "metadata": {
        "id": "dabd9468"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=0.5, random_state=0)\n",
        "scores = []\n",
        "for k in range(2, 6):\n",
        "    kmeans = KMeans(n_clusters=k).fit(X)\n",
        "    score = silhouette_score(X, kmeans.labels_)\n",
        "    scores.append(score)\n",
        "\n",
        "plt.bar(range(2, 6), scores)\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Scores for k = 2 to 5\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c56f322",
      "metadata": {
        "id": "4c56f322"
      },
      "source": [
        "**10. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28fd0172",
      "metadata": {
        "id": "28fd0172"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "linked = linkage(iris.data, method='average')\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked, labels=iris.target)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram (Average Linkage)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2aff709",
      "metadata": {
        "id": "e2aff709"
      },
      "source": [
        "**11. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8d448b",
      "metadata": {
        "id": "aa8d448b"
      },
      "outputs": [],
      "source": [
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "kmeans = KMeans(n_clusters=3).fit(X)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, s=30, cmap='viridis')\n",
        "plt.title(\"KMeans Decision Boundaries\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d84e826",
      "metadata": {
        "id": "8d84e826"
      },
      "source": [
        "**12. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba242c1",
      "metadata": {
        "id": "9ba242c1"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(digits.data)\n",
        "db = DBSCAN(eps=5, min_samples=5).fit(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=db.labels_, cmap='tab10')\n",
        "plt.title(\"DBSCAN on Digits (t-SNE reduced)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ab3bb0",
      "metadata": {
        "id": "b8ab3bb0"
      },
      "source": [
        "**13. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ee4e8e",
      "metadata": {
        "id": "e9ee4e8e"
      },
      "outputs": [],
      "source": [
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(\"Agglomerative Clustering with Complete Linkage\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14fec67",
      "metadata": {
        "id": "e14fec67"
      },
      "source": [
        "**14. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a4af8d",
      "metadata": {
        "id": "a1a4af8d"
      },
      "outputs": [],
      "source": [
        "inertias = []\n",
        "for k in range(2, 7):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(2, 7), inertias, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"Inertia for KMeans (K=2 to 6)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746b561f",
      "metadata": {
        "id": "746b561f"
      },
      "source": [
        "**15. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "226d2ec1",
      "metadata": {
        "id": "226d2ec1"
      },
      "outputs": [],
      "source": [
        "X, _ = make_circles(n_samples=300, noise=0.05, factor=0.5)\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma')\n",
        "plt.title(\"Agglomerative Clustering on make_circles\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f38612",
      "metadata": {
        "id": "24f38612"
      },
      "source": [
        "**16. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2382a831",
      "metadata": {
        "id": "2382a831"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = StandardScaler().fit_transform(data.data)\n",
        "db = DBSCAN(eps=1.5, min_samples=5).fit(X)\n",
        "n_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)\n",
        "print(\"Number of clusters (excluding noise):\", n_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53026f1",
      "metadata": {
        "id": "c53026f1"
      },
      "source": [
        "**17. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e67681",
      "metadata": {
        "id": "b7e67681"
      },
      "outputs": [],
      "source": [
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "kmeans = KMeans(n_clusters=4, random_state=0).fit(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='Set1')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='black')\n",
        "plt.title(\"Cluster Centers with make_blobs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34cd2099",
      "metadata": {
        "id": "34cd2099"
      },
      "source": [
        "**18. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1376ce07",
      "metadata": {
        "id": "1376ce07"
      },
      "outputs": [],
      "source": [
        "X = iris.data\n",
        "db = DBSCAN(eps=0.5, min_samples=5).fit(X)\n",
        "n_noise = list(db.labels_).count(-1)\n",
        "print(\"Number of noise samples identified:\", n_noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb97b9a2",
      "metadata": {
        "id": "bb97b9a2"
      },
      "source": [
        "**19. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af6db690",
      "metadata": {
        "id": "af6db690"
      },
      "outputs": [],
      "source": [
        "X, _ = make_moons(n_samples=300, noise=0.1)\n",
        "kmeans = KMeans(n_clusters=2).fit(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='coolwarm')\n",
        "plt.title(\"KMeans on make_moons (non-linear data)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a92aa98",
      "metadata": {
        "id": "5a92aa98"
      },
      "source": [
        "**20. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18338474",
      "metadata": {
        "id": "18338474"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "X_pca = PCA(n_components=3).fit_transform(digits.data)\n",
        "kmeans = KMeans(n_clusters=10).fit(X_pca)\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=kmeans.labels_, cmap='tab10')\n",
        "plt.title(\"3D PCA of Digits with KMeans Clustering\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3833aee",
      "metadata": {
        "id": "f3833aee"
      },
      "source": [
        "**21. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c374518b",
      "metadata": {
        "id": "c374518b"
      },
      "outputs": [],
      "source": [
        "X, _ = make_blobs(n_samples=300, centers=5, random_state=42)\n",
        "kmeans = KMeans(n_clusters=5).fit(X)\n",
        "score = silhouette_score(X, kmeans.labels_)\n",
        "print(\"Silhouette Score for KMeans with 5 centers:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8280101",
      "metadata": {
        "id": "a8280101"
      },
      "source": [
        "**22. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2b7ea00",
      "metadata": {
        "id": "c2b7ea00"
      },
      "outputs": [],
      "source": [
        "X = StandardScaler().fit_transform(load_breast_cancer().data)\n",
        "X_pca = PCA(n_components=2).fit_transform(X)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=2)\n",
        "labels = agg.fit_predict(X_pca)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='Accent')\n",
        "plt.title(\"Agglomerative Clustering on Breast Cancer (PCA Reduced)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce49749",
      "metadata": {
        "id": "3ce49749"
      },
      "source": [
        "**23. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acebc0ce",
      "metadata": {
        "id": "acebc0ce"
      },
      "outputs": [],
      "source": [
        "X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"KMeans\")\n",
        "plt.scatter(X[:,0], X[:,1], c=KMeans(n_clusters=2).fit_predict(X), cmap='cool')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"DBSCAN\")\n",
        "plt.scatter(X[:,0], X[:,1], c=DBSCAN(eps=0.2).fit_predict(X), cmap='cool')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1a9e27",
      "metadata": {
        "id": "ae1a9e27"
      },
      "source": [
        "**24. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c43193",
      "metadata": {
        "id": "a2c43193"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_samples\n",
        "import seaborn as sns\n",
        "\n",
        "X = iris.data\n",
        "kmeans = KMeans(n_clusters=3).fit(X)\n",
        "sil_values = silhouette_samples(X, kmeans.labels_)\n",
        "\n",
        "plt.bar(range(len(sil_values)), sil_values)\n",
        "plt.title(\"Silhouette Coefficients for Iris Dataset\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Silhouette Coefficient\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1532d3d1",
      "metadata": {
        "id": "1532d3d1"
      },
      "source": [
        "**25. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4210fc06",
      "metadata": {
        "id": "4210fc06"
      },
      "outputs": [],
      "source": [
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6)\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(\"Agglomerative Clustering (Average Linkage)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df21208",
      "metadata": {
        "id": "2df21208"
      },
      "source": [
        "**26. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9752cd",
      "metadata": {
        "id": "2e9752cd"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "X = StandardScaler().fit_transform(load_wine().data)\n",
        "df = pd.DataFrame(X[:, :4], columns=['feat1','feat2','feat3','feat4'])\n",
        "df['cluster'] = KMeans(n_clusters=3).fit_predict(X)\n",
        "\n",
        "sns.pairplot(df, hue='cluster')\n",
        "plt.suptitle(\"Seaborn Pairplot of Clusters (First 4 Features)\", y=1.02)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c28663",
      "metadata": {
        "id": "78c28663"
      },
      "source": [
        "**27. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ed0bb20",
      "metadata": {
        "id": "5ed0bb20"
      },
      "outputs": [],
      "source": [
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.7)\n",
        "db = DBSCAN(eps=0.8, min_samples=5).fit(X)\n",
        "\n",
        "labels = db.labels_\n",
        "print(\"Cluster counts (including noise):\", dict(zip(*np.unique(labels, return_counts=True))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ab6530",
      "metadata": {
        "id": "42ab6530"
      },
      "source": [
        "**28. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202f268b",
      "metadata": {
        "id": "202f268b"
      },
      "outputs": [],
      "source": [
        "X_tsne = TSNE(n_components=2).fit_transform(digits.data)\n",
        "agg = AgglomerativeClustering(n_clusters=10).fit(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=agg.labels_, cmap='tab10')\n",
        "plt.title(\"Agglomerative Clustering on Digits (t-SNE Reduced)\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}