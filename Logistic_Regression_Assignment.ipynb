{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6001b512",
      "metadata": {
        "id": "6001b512"
      },
      "source": [
        "## Theoretical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da80303b",
      "metadata": {
        "id": "da80303b"
      },
      "source": [
        "**Q1. What is Logistic Regression, and how does it differ from Linear Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aab1dcf",
      "metadata": {
        "id": "0aab1dcf"
      },
      "source": [
        "Logistic Regression is a classification algorithm used to predict the probability of a binary outcome (0 or 1). Unlike Linear Regression which predicts a continuous output, Logistic Regression applies the logistic (sigmoid) function to map linear combinations of inputs to a probability between 0 and 1. This makes it suitable for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed56a905",
      "metadata": {
        "id": "ed56a905"
      },
      "source": [
        "**Q2. What is the mathematical equation of Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23614a14",
      "metadata": {
        "id": "23614a14"
      },
      "source": [
        "The core equation is:\n",
        "$$P(y=1|\\mathbf{x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}$$\n",
        "where $\\beta_0$ is the intercept and $\\beta_i$ are the coefficients for input features $x_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc4ca8b",
      "metadata": {
        "id": "dbc4ca8b"
      },
      "source": [
        "**Q3. Why do we use the Sigmoid function in Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04efc414",
      "metadata": {
        "id": "04efc414"
      },
      "source": [
        "The sigmoid (logistic) function $$\\sigma(z)=\\frac{1}{1+e^{-z}}$$ maps any real-valued input $z$ to the (0,1) interval, allowing interpretation of outputs as probabilities. It also has a smooth gradient, making optimization via gradient descent feasible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cecca94e",
      "metadata": {
        "id": "cecca94e"
      },
      "source": [
        "**Q4. What is the cost function of Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490954fa",
      "metadata": {
        "id": "490954fa"
      },
      "source": [
        "The cost function is the log-loss (cross-entropy):\n",
        "$$J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(h(x^{(i)})) + (1-y^{(i)}) \\log(1 - h(x^{(i)}))]$$\n",
        "where $h(x)=\\sigma(\\beta^T x)$ and $m$ is the number of samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ba8a151",
      "metadata": {
        "id": "7ba8a151"
      },
      "source": [
        "**Q5. What is Regularization in Logistic Regression? Why is it needed?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1b4ee0",
      "metadata": {
        "id": "4b1b4ee0"
      },
      "source": [
        "Regularization adds a penalty on large coefficients to the cost function to prevent overfitting. In Logistic Regression, L1 (Lasso) and L2 (Ridge) penalties are used. It is needed when the model fits the training data too closely and fails to generalize."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14833c8e",
      "metadata": {
        "id": "14833c8e"
      },
      "source": [
        "**Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073dce5e",
      "metadata": {
        "id": "073dce5e"
      },
      "source": [
        "- **Lasso (L1)** adds $\\lambda \\sum_i |\\beta_i|$, encouraging sparsity (some coefficients become zero).\n",
        "- **Ridge (L2)** adds $\\lambda \\sum_i \\beta_i^2$, shrinking coefficients but not enforcing zeros.\n",
        "- **Elastic Net** combines both: $\\lambda_1 \\sum |β_i| + λ_2 \\sum β_i^2$, balancing sparsity and coefficient shrinkage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953c95e3",
      "metadata": {
        "id": "953c95e3"
      },
      "source": [
        "**Q7. When should we use Elastic Net instead of Lasso or Ridge?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0bdebc",
      "metadata": {
        "id": "dd0bdebc"
      },
      "source": [
        "Use Elastic Net when there are many correlated features. Lasso may arbitrarily pick one feature, whereas Elastic Net can include groups of correlated features while still performing variable selection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2129ef78",
      "metadata": {
        "id": "2129ef78"
      },
      "source": [
        "**Q8. What is the impact of the regularization parameter (λ) in Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c6a115",
      "metadata": {
        "id": "38c6a115"
      },
      "source": [
        "A larger λ increases the penalty on coefficients, leading to smaller weights and simpler models (more bias, less variance). A smaller λ reduces penalty, allowing more complex models (less bias, more variance)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1728c365",
      "metadata": {
        "id": "1728c365"
      },
      "source": [
        "**Q9. What are the key assumptions of Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae093179",
      "metadata": {
        "id": "ae093179"
      },
      "source": [
        "1. The log-odds (logit) of the outcome is a linear combination of input features.\n",
        "2. Observations are independent.\n",
        "3. No multicollinearity among predictors.\n",
        "4. Large sample size for stable estimates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f49d2786",
      "metadata": {
        "id": "f49d2786"
      },
      "source": [
        "**Q10. What are some alternatives to Logistic Regression for classification tasks?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c6626c",
      "metadata": {
        "id": "26c6626c"
      },
      "source": [
        "Decision Trees, Random Forests, Support Vector Machines, Naive Bayes, k-Nearest Neighbors, Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71a3a8e",
      "metadata": {
        "id": "d71a3a8e"
      },
      "source": [
        "**Q11. What are Classification Evaluation Metrics?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6880659c",
      "metadata": {
        "id": "6880659c"
      },
      "source": [
        "Common metrics include Accuracy, Precision, Recall, F1-Score, ROC-AUC, Log-Loss, and Confusion Matrix components (True/False Positives/Negatives)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87e7cf27",
      "metadata": {
        "id": "87e7cf27"
      },
      "source": [
        "**Q12. How does class imbalance affect Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e7a267",
      "metadata": {
        "id": "37e7a267"
      },
      "source": [
        "In imbalanced datasets, the model may be biased toward the majority class, leading to misleading accuracy. Solutions include using class weights, resampling (oversampling minority or undersampling majority), or specialized metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ed0484",
      "metadata": {
        "id": "d8ed0484"
      },
      "source": [
        "**Q13. What is Hyperparameter Tuning in Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6b6dd01",
      "metadata": {
        "id": "e6b6dd01"
      },
      "source": [
        "The process of selecting optimal hyperparameters (like regularization strength C, type of penalty, solver) using techniques such as GridSearchCV, RandomizedSearchCV, or Bayesian optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fc0187c",
      "metadata": {
        "id": "4fc0187c"
      },
      "source": [
        "**Q14. What are different solvers in Logistic Regression? Which one should be used?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "497ff372",
      "metadata": {
        "id": "497ff372"
      },
      "source": [
        "Common solvers: `liblinear`, `newton-cg`, `lbfgs`, `sag`, and `saga`.\n",
        "- Use `liblinear` for small datasets and L1 penalty.\n",
        "- `lbfgs` and `newton-cg` are good for multiclass and L2 penalty.\n",
        "- `sag` and `saga` are fast for large datasets; `saga` supports L1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7391b3",
      "metadata": {
        "id": "9c7391b3"
      },
      "source": [
        "**Q15. How is Logistic Regression extended for multiclass classification?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7dea9a7",
      "metadata": {
        "id": "e7dea9a7"
      },
      "source": [
        "Via One-vs-Rest (OvR), training one binary classifier per class, or using Softmax (multinomial logistic regression) which generalizes the sigmoid to multiple classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fc01c4",
      "metadata": {
        "id": "b1fc01c4"
      },
      "source": [
        "**Q16. What are the advantages and disadvantages of Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a35ca75",
      "metadata": {
        "id": "7a35ca75"
      },
      "source": [
        "- **Advantages**: Simple, interpretable, efficient, outputs probabilities, no complex tuning.\n",
        "- **Disadvantages**: Assumes linearity in log-odds, prone to underperform on complex patterns, sensitive to outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30634508",
      "metadata": {
        "id": "30634508"
      },
      "source": [
        "**Q17. What are some use cases of Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11770451",
      "metadata": {
        "id": "11770451"
      },
      "source": [
        "Medical diagnosis (disease presence), credit scoring, spam detection, customer churn prediction, binary event forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c40961a5",
      "metadata": {
        "id": "c40961a5"
      },
      "source": [
        "**Q18. What is the difference between Softmax Regression and Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ce1535",
      "metadata": {
        "id": "55ce1535"
      },
      "source": [
        "Softmax Regression (multinomial) generalizes binary Logistic Regression to multiple classes by using the softmax function to produce a probability distribution over classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53aef057",
      "metadata": {
        "id": "53aef057"
      },
      "source": [
        "**Q19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b06f526",
      "metadata": {
        "id": "0b06f526"
      },
      "source": [
        "OvR is simpler and can be faster on certain solvers, but Softmax considers all classes jointly and may yield better probability estimates when classes are mutually exclusive."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03dab76d",
      "metadata": {
        "id": "03dab76d"
      },
      "source": [
        "**Q20. How do we interpret coefficients in Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff7ffc0",
      "metadata": {
        "id": "bff7ffc0"
      },
      "source": [
        "Coefficients represent the change in the log-odds of the outcome per unit increase in the predictor. Exponentiating a coefficient gives the odds ratio for that feature."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ecaccfb",
      "metadata": {
        "id": "5ecaccfb"
      },
      "source": [
        "## Practical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f230d20d",
      "metadata": {
        "id": "f230d20d"
      },
      "source": [
        "**Q1. Write a Python program to load a dataset, split it into training and testing sets, apply Logistic Regression, and print the model accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4f685e2",
      "metadata": {
        "id": "b4f685e2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4088414",
      "metadata": {
        "id": "d4088414"
      },
      "source": [
        "**Q2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9091b6d9",
      "metadata": {
        "id": "9091b6d9"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"L1 Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5b29b5",
      "metadata": {
        "id": "ad5b29b5"
      },
      "source": [
        "**Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad60c559",
      "metadata": {
        "id": "ad60c559"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(penalty='l2')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"L2 Accuracy:\", model.score(X_test, y_test))\n",
        "print(\"Coefficients:\", model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19160cb",
      "metadata": {
        "id": "f19160cb"
      },
      "source": [
        "**Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60de0525",
      "metadata": {
        "id": "60de0525"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"ElasticNet Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8987bc",
      "metadata": {
        "id": "0a8987bc"
      },
      "source": [
        "**Q5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb072c7",
      "metadata": {
        "id": "4cb072c7"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"OvR Accuracy:\", model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cda364f",
      "metadata": {
        "id": "7cda364f"
      },
      "source": [
        "**Q6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f2298af",
      "metadata": {
        "id": "7f2298af"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1594a537",
      "metadata": {
        "id": "1594a537"
      },
      "source": [
        "**Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b82f4246",
      "metadata": {
        "id": "b82f4246"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "print(\"Average Stratified K-Fold Accuracy:\", scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc48ddb",
      "metadata": {
        "id": "9cc48ddb"
      },
      "source": [
        "**Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b0efe4",
      "metadata": {
        "id": "39b0efe4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"your_dataset.csv\")  # Replace with your file\n",
        "X = df.drop(\"target\", axis=1)  # Replace with actual target column\n",
        "y = df[\"target\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}